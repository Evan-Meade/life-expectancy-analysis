---
title: "Life Expectancy Project"
author: "Abraca-Data Team"
date: "4/15/2021"
output:
  html_document: default
  pdf_document: default
---
### Importing Libraries
```{r message = FALSE}
library(tidyverse)
library(corrplot)
library(janitor)
library(gganimate)
library(ggpubr)
library(ggthemes)
library(scales)
library(car)
```

```{r}
theme_set(theme_dark())
```

### Importing Data
```{r message = FALSE}
life <- read_csv("Life Expectancy Data.csv") %>%
  clean_names() %>%
  select(country, year, status, life_expectancy, alcohol, percentage_expenditure,
         bmi, gdp, hiv_aids, thinness_1_19_years, thinness_5_9_years, income_composition_of_resources, schooling)
```

### Preprocessing
```{r}
# Checking the percentage of missing values
na_pct <-sapply(life, function(x) sum(length(which(is.na(x))))/100)
na_pct <- data.frame(na_pct)

# Dropping all missing values in the data frame
life <- life %>% drop_na()
```

### EDA
```{r fit.height = 5, fit.width = 15}
corrplot(cor(life[ , -c(1:3)]),
         type="upper",
         method = "number", 
         addCoefasPercent = FALSE)
```

## Given these predictors, how accurately can one predict life expectancy with a linear model?
```{r}
model <- lm(life_expectancy ~ 
              (alcohol + percentage_expenditure + bmi + gdp + schooling + income_composition_of_resources + hiv_aids
               + thinness_1_19_years + thinness_5_9_years),
            data = life)

summary(model)
residualPlot(model, main = "Residual Plot of the Overall Model")
hist(model$residuals, breaks = 50, col = 'red', main = "Histogram of residuals - Overall Model", xlab = "Residuals Values")
```
For this analysis there are nine features within the data set that we would like to choose to predict the Life Expectancy, which is our response variable. Now, we are going to perform a linear model and evaluate its accuracy to determine if it is a good regression model or not based on our assumptions of having a high magnitude correlation with the response variable, otherwise the Life expectancy variable. After evaluating the linear model we can observe that the accuracy of this model is 79.99% from the Adjusted R-squared from the predictor variables that we have selected. From the linear model, we can also observe that the linear model has a few variables that are not that significant since they have a higher p-value which suggests that the changes in the predictor variables are not associated with the changes in the response variable. And the predictor variables that have no significance to the model are `percentage_expenditure`, `gdp`, `thinness_1_19_years` and `thinness_5_9_years`. In the next part of the analysis we might need to consider reducing the dimensionality to observe if the model improves in terms of predictive power.

Now, after we have our linear model, we are going to check for its residuals plot to see if there is not any non-linear relationships. Here we see that linearity seems to hold reasonably well, as the blue line is close to the dashed line, specifically near 0 in the x-axis. We can also note the homocedasticity: as we move to the right on the x-axis, it is equally spread residuals around a horizontal line without any distinct patterns, which is a good indication that we do not have any non-linear relationships. Finally there some points within the residual plot where they could possibly may be outliers. But overall, our linear model is good and reasonable. In addition, we have constructed a histogram of the residuals from the linear model and as we can observe we can see that the variance is normally distributed.

## Are any of the predictors correlated with each other? If so, can the dimensionality of the dataset be reduced while retaining predictive power?
From the previous question that has been answered we saw that there has been a few predictors that we have selected for this model have not been quite significant (meaning it was over the p-value of 5% or 0.05). We hypothesized that if we made a decision of removing that specific variables from the model, would it increase and still retain its predictive power? or have some loss with the new modified model if we were to reduce the dimensionality of the dataset?

```{r}
model2 <- lm(life_expectancy ~ (alcohol + bmi + schooling + income_composition_of_resources + hiv_aids +   thinness_1_19_years),
            data = life)

summary(model2)
residualPlot(model2, main = "Residual Plot of the Reduced Model")
hist(model2$residuals, breaks = 50, col = 'red', main = "Histogram of residuals - Reduced Model", xlab = "Residuals Values")
```
After making certain adjustments/changes to the model we can obeserve after making the removal of certain insignificant variables from the model we can conclude that it did not affect the accuracy of the model that much and still retains its predictive po25% (lost about 0.74% of predictive power). As previously answered from the question before, our linear model is good and reasonable.

```{r}
developed <- data.frame(life[which(life['status'] == "Developed"),])
developing <- data.frame(life[which(life['status'] != "Developed"),])
```


```{r}
model_developed <- lm(life_expectancy ~ (alcohol + bmi + schooling +  percentage_expenditure + gdp +income_composition_of_resources  +  hiv_aids + thinness_1_19_years + thinness_5_9_years),
            data = developed)
summary(model_developed)
residualPlot(model_developed, main = "Residual Plot of the Developed Model")
hist(model_developed$residuals, breaks = 50, col = 'red', main = "Histogram of residuals - Developed Model", xlab = "Residuals Values")
```


```{r}
model_developing <- lm(life_expectancy ~ (alcohol + bmi + schooling +  percentage_expenditure + gdp +income_composition_of_resources + hiv_aids +   thinness_1_19_years + thinness_5_9_years),
            data = developing)
summary(model_developing)
residualPlot(model_developing, main = "Residual Plot of the Developing Model")
hist(model_developing$residuals, breaks = 50, col = 'red', main = "Histogram of residuals - Developing Model", xlab = "Residuals Values")
```


